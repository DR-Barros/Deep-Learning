{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b921b3f-4258-4a38-b14c-e9befafbeae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31b16d0-b04a-4462-9ba6-e20a162691ad",
   "metadata": {},
   "source": [
    "# TAREA 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea12857-e114-4b14-b8c6-8c453023f7b1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "Usamos un dataset pequeñito de analisis de sentimiento en tweets. Cada tweet viene con una etiqueta entre `positive`, `negative` y `neutral`. **Mismo dataset que por la Tarea 4**. \n",
    "\n",
    "**Vamos a ver como usar:**\n",
    "* Entrenar un modelo con LoRA con `peft` \n",
    "* Comparar las performancias, el tiempo de entrenamiento, y la memoria del GPU usado\n",
    "* Quantizacion con `BitsAndBytes` \n",
    "\n",
    "## Se puede utilizar LLM para ayudarse! \n",
    "* Mirar a [un ejemplo](https://chatgpt.com/share/d534833e-bd2c-40c1-81eb-34818b195cac) de como pedir las respuestas a un LLM. Mejor si entenden lo que hagan.\n",
    "* **Cuidado**: con librerias muy nuevas, se puede ser un poco dificil de usar LLMs\n",
    "\n",
    "#### Para cada entrenamiento \n",
    "* Informa de las curvas de pérdida y precisión en el entrenamiento y la validación.\n",
    "* Detén el entrenamiento en función de los resultados obtenidos en el conjunto de validación.\n",
    "* Utiliza una tasa de aprendizaje pequeña para que puedas ver cómo disminuye la pérdida en las distintas curvas.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c69aa7-7fbe-4ea9-a578-d2d1dc469305",
   "metadata": {},
   "source": [
    "## Train a model using LoRA \n",
    "\n",
    "Train a Model using peft and lora, which normal y cannot be run without it. \n",
    "\n",
    "* Load a Large Generative Model such as [`google/flan-t5-xl`](https://huggingface.co/google/flan-t5-xl) or `google/flan-t5-large`\n",
    "* If needed transform the dataset so that the labels are encoded as text\n",
    "* Be careful of encoding well the input text as it is a targeted sentiment analysis task\n",
    "* Use the `peft` library from HuggingFace\n",
    "* See the difference with `model.print_trainable_parameters()`\n",
    "* Train the model with LoRA\n",
    "* Verify it cannot be trained otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d4520d-b32c-4b1d-b532-c3e0ae39243e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98684e3c-9dcc-49cb-aad6-3c93cb1a2bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057ceeb3-11db-4855-bb0d-0a5e6fd851b4",
   "metadata": {},
   "source": [
    "### Preguntas\n",
    "\n",
    "**Cual es el principo de LoRA?**\n",
    "* Porque se usan menos parametros?\n",
    "* Que significa el rango del adaptador?\n",
    "\n",
    "**Cuales son las diferencias sin y con usar LoRA?** \n",
    "* Se puede entrenar sin LoRA? \n",
    "* Cae en la memoria?\n",
    "* Es mas lento/rapido?\n",
    "* Como cambian las performancias?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92aa8101-9da5-490c-9348-09304cd9911b",
   "metadata": {},
   "source": [
    "**Respuestas**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46ec33a-9909-4dee-a453-839532b2e936",
   "metadata": {},
   "source": [
    "# Load a model using quantization\n",
    "\n",
    "* Use `BitsAndBytes` in order to load a model that you could not load without quantization\n",
    "* Print the quantity of memory the model takes in the GPU in each case (8 bits, 4 bits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "212ff82b-031f-4379-bf04-05ca0125d549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1ddde5-b0dd-4094-8665-b220fa38a8c3",
   "metadata": {},
   "source": [
    "## Preguntas\n",
    "\n",
    "* Cuánta memoria se puede ahorrar?\n",
    "* Cuál es el interés de esta técnica?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0598af91-0c89-409b-8fc1-b9bd6b02e62e",
   "metadata": {},
   "source": [
    "**Respuestas**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
